import sqlite3
import os
import time
import random
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from colorama import Fore, Style, init
init(autoreset=True)

import requests
from datetime import datetime

def charger_config(path="config/conf.txt"):
    config = {
        "db_path": "db/scraper_data.db",
        "delay": 1000,
        "max_depth": 2,
        "expiration": 30,
        "log_file": "log.txt"
    }
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for ligne in f:
                if "=" in ligne:
                    cle, val = ligne.strip().split("=", 1)
                    config[cle.strip()] = val.strip()
    config["delay"] = int(config.get("delay", 1000))
    config["max_depth"] = int(config.get("max_depth", 2))
    config["expiration"] = int(config.get("expiration", 30))
    return config

def log(msg, path):
    horodatage = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    line = f"{horodatage} {msg}"
    with open(path, "a", encoding="utf-8") as f:
        f.write(line + "\n")
    print(Fore.LIGHTGREEN_EX + line)

def charger_liste(path):
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return [l.strip() for l in f if l.strip()]

def nettoyer_texte(html):
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style"]):
        tag.decompose()
    return soup.get_text(separator=" ", strip=True)

def extraire_liens(base_url, html):
    soup = BeautifulSoup(html, "html.parser")
    liens = set()
    for a in soup.find_all("a", href=True):
        href = urljoin(base_url, a["href"])
        if href.startswith("http"):
            href = href.split("#")[0]
            liens.add(href)
    return list(liens)

def domaine(url):
    return urlparse(url).netloc

def corriger_encodage_texte(texte):
    try:
        return texte.encode('latin1').decode('utf-8')
    except UnicodeEncodeError:
        return texte
    except UnicodeDecodeError:
        return texte

def lire_telegram_config(path="config/telegram_secret.txt"):
    try:
        with open(path, "r", encoding="utf-8") as f:
            lignes = f.readlines()
        config = {}
        for ligne in lignes:
            if "=" in ligne:
                cle, valeur = ligne.strip().split("=", 1)
                config[cle.strip()] = valeur.strip()
        return config.get("bot_token"), config.get("chat_id")
    except Exception as e:
        print(f"‚ùå Erreur lecture telegram_secret.txt : {e}")
        return None, None

def envoyer_alerte_telegram(message, bot_token, chat_id):
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    data = {"chat_id": chat_id, "text": message}
    try:
        r = requests.post(url, data=data, timeout=20)
        if r.ok:
            print(f"‚úÖ Alerte Telegram envoy√©e : {message}")
        else:
            print(f"‚ùå Erreur Telegram : {r.status_code} - {r.text}")
    except Exception as e:
        print(f"üö´ √âchec envoi Telegram : {e}")

# === Initialisation ===
config = charger_config()
bot_token, chat_id = lire_telegram_config()
log_file = config["log_file"]
db_path = config["db_path"]

log("------- üèÅ D√©marrage de Ninja -------", log_file)
conn = sqlite3.connect(db_path)
cur = conn.cursor()
log(f"Connexion √† la base : {db_path}", log_file)

# Chargement des mots-cl√©s
mots_cles = charger_liste("config/keywords.txt")
cur.execute("DELETE FROM MOTS_CLES")
for mot in mots_cles:
    cur.execute("INSERT OR IGNORE INTO MOTS_CLES (mot) VALUES (?)", (mot,))
conn.commit()
log(f"{len(mots_cles)} mot(s)-cl√©(s) charg√©(s)", log_file)

# Chargement des sites
sites = charger_liste("config/url.txt")
cur.execute("DELETE FROM SITES_A_VISITER")
for site in sites:
    cur.execute("INSERT OR IGNORE INTO SITES_A_VISITER (url) VALUES (?)", (site,))
conn.commit()
log(f"{len(sites)} site(s) √† visiter charg√©(s)", log_file)

# Insertion ou r√©activation dans PAGES_A_VISITER
for url in sites:
    cur.execute("""
        INSERT OR IGNORE INTO PAGES_A_VISITER (url, profondeur, date_visite)
        VALUES (?, 0, NULL)
    """, (url,))
    cur.execute("""
        UPDATE PAGES_A_VISITER SET date_visite = NULL, profondeur = 0
        WHERE url = ?
    """, (url,))
conn.commit()

# Nettoyage des PAGES_A_VISITER hors domaine
cur.execute("SELECT url FROM PAGES_A_VISITER")
pages = cur.fetchall()
urls_a_supprimer = [
    (url,) for (url,) in pages
    if not any(url.startswith(site) for site in sites)
]
if urls_a_supprimer:
    cur.executemany("DELETE FROM PAGES_A_VISITER WHERE url = ?", urls_a_supprimer)
    log(f"üßπ {len(urls_a_supprimer)} URL supprim√©e(s) de PAGES_A_VISITER (hors domaine)", log_file)
else:
    log("üßπ Aucune URL hors domaine √† supprimer dans PAGES_A_VISITER", log_file)
conn.commit()

# Nettoyage des contenus r√©cup√©r√©s hors domaine
cur.execute("SELECT url FROM CONTENU_RECUPERE")
contenus = cur.fetchall()
urls_contenus_a_supprimer = [
    (url,) for (url,) in contenus
    if not any(url.startswith(site) for site in sites)
]
if urls_contenus_a_supprimer:
    cur.executemany("DELETE FROM CONTENU_RECUPERE WHERE url = ?", urls_contenus_a_supprimer)
    log(f"üßπ {len(urls_contenus_a_supprimer)} contenu(s) supprim√©(s) de CONTENU_RECUPERE (hors domaine)", log_file)
else:
    log("üßπ Aucun contenu hors domaine √† supprimer dans CONTENU_RECUPERE", log_file)
conn.commit()

# Message Telegram au d√©marrage
if bot_token and chat_id:
    envoyer_alerte_telegram("üèÅ Scraper NINJA d√©marr√©", bot_token, chat_id)

# === Boucle d'exploration ===
expiration_minutes = config["expiration"]
delay_seconds = config["delay"] / 1000
max_depth = config["max_depth"]

while True:
    cur.execute("""
        SELECT url, profondeur FROM PAGES_A_VISITER
        WHERE date_visite IS NULL
    """)
    non_visitees = cur.fetchall()

    cur.execute("""
        SELECT url, profondeur FROM PAGES_A_VISITER
        WHERE date_visite IS NOT NULL
          AND datetime(date_visite) <= datetime('now', ?)
        ORDER BY RANDOM()
    """, (f"-{expiration_minutes} minutes",))
    anciennes = cur.fetchall()

    lignes = non_visitees + anciennes
    log(f"üìã Pages √† explorer : {len(non_visitees)} non visit√©es + {len(anciennes)} anciennes (>{expiration_minutes} min)", log_file)

    if not lignes:
        log("üõë Aucune page √† visiter. Fin du scraping.", log_file)
        cur.execute("SELECT COUNT(*) FROM CONTENU_RECUPERE")
        contenus = cur.fetchone()[0]
        cur.execute("SELECT COUNT(*) FROM PAGE_VISITEE")
        visitees = cur.fetchone()[0]
        log(f"üîö Statistiques : {visitees} pages visit√©es ‚Äì {contenus} contenus collect√©s", log_file)
        break

    for url, profondeur in lignes:
        maintenant = datetime.now()
        print(Fore.LIGHTCYAN_EX + f"üîç Exploration : {url} (profondeur {profondeur})")

        try:
            response = requests.get(url, timeout=10)
            html = response.text
            texte = corriger_encodage_texte(nettoyer_texte(html))

            horodatage = maintenant.strftime("%Y-%m-%d %H:%M:%S")
            cur.execute("UPDATE PAGES_A_VISITER SET date_visite = ? WHERE url = ?", (horodatage, url))
            cur.execute("INSERT OR IGNORE INTO PAGE_VISITEE (url, profondeur, date_visite) VALUES (?, ?, ?)", (url, profondeur, horodatage))

            mots_trouves = [mot for mot in mots_cles if mot.lower() in texte.lower()]
            if mots_trouves:
                titre = BeautifulSoup(html, "html.parser").title
                titre = corriger_encodage_texte(titre.text.strip()) if titre else "(sans titre)"

                cur.execute("SELECT 1 FROM CONTENU_RECUPERE WHERE url = ? AND texte = ?", (url, texte[:1000]))
                existe_deja = cur.fetchone()

                if not existe_deja:
                    cur.execute("""
                        INSERT INTO CONTENU_RECUPERE (date, mots_cles, titre, texte, site, url)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        horodatage,
                        ", ".join(mots_trouves),
                        titre,
                        texte[:1000],
                        domaine(url),
                        url
                    ))
                    log(f"‚ö†Ô∏è  Mot(s)-cl√© d√©tect√©(s) : {', '.join(mots_trouves)} dans {titre}", log_file)

                    if bot_token and chat_id:
                        message = f"üîé Mot-cl√© d√©tect√© : {', '.join(mots_trouves)}\nüåê {titre}"
                        envoyer_alerte_telegram(message, bot_token, chat_id)
                else:
                    log(f"üîÅ Aucun ajout : contenu d√©j√† connu pour {url}", log_file)

            if profondeur < max_depth:
                for lien in extraire_liens(url, html):
                    if domaine(lien) != domaine(url):
                        continue
                    if "." in lien.split("/")[-1] and not lien.endswith(".html"):
                        continue
                    cur.execute("SELECT 1 FROM PAGES_A_VISITER WHERE url = ?", (lien,))
                    if not cur.fetchone():
                        cur.execute("""
                            INSERT INTO PAGES_A_VISITER (url, profondeur, date_visite)
                            VALUES (?, ?, NULL)
                        """, (lien, profondeur + 1))
                        print(Fore.CYAN + " Ajout d'une page √† visiter :" + url)

            conn.commit()
            time.sleep(delay_seconds)

        except Exception as e:
            log(f"Erreur sur {url} : {e}", log_file)
            cur.execute("UPDATE PAGES_A_VISITER SET date_visite = ? WHERE url = ?", (maintenant.strftime("%Y-%m-%d %H:%M:%S"), url))
            cur.execute("INSERT OR IGNORE INTO PAGE_VISITEE (url, profondeur, date_visite) VALUES (?, ?, ?)", (url, profondeur, maintenant.strftime("%Y-%m-%d %H:%M:%S")))
            conn.commit()
            time.sleep(delay_seconds)
